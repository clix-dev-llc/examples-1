apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: terasort
spec:
  type: Scala
  mode: cluster
  image: mesosphere/spark:spark-3.0.0-hadoop-2.9-k8s-rc3
  imagePullPolicy: IfNotPresent
  mainClass: sorting.SortingApp
  mainApplicationFile: "https://kudo-spark.s3-us-west-2.amazonaws.com/spark-scala-tests-3.0.0-20200819.jar"
  arguments:
    - <input bucket, ex s3a://terasort-data>>
    - <output bucket, ex s3a://terasort-scratch/spark-terasort-sorted>
  sparkConf:
    "spark.kubernetes.allocation.batch.size": "500"
    "spark.scheduler.maxRegisteredResourcesWaitingTime": "3600s"
    "spark.scheduler.minRegisteredResourcesRatio": "1.0"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2"
    "spark.hadoop.fs.s3a.endpoint": "s3.us-west-2.amazonaws.com"
    "spark.hadoop.fs.s3a.access.key": ""
    "spark.hadoop.fs.s3a.secret.key": ""
    "spark.driver.maxResultSize": "64g"
    "spark.memory.fraction": "0.1"
    "spark.kubernetes.memoryOverheadFactor": "0.1"
    "spark.kubernetes.executor.request.cores": "1"
    "spark.kubernetes.executor.limit.cores": "1"
    "spark.kubernetes.driver.request.cores": "1"
    "spark.kubernetes.driver.limit.cores": "1"
  sparkVersion: "3.0.0"
  restartPolicy:
    type: Never
  driver:
    memory: "4g"
    labels:
      version: 3.0.0
      metrics-exposed: "true"
      app: sparky
    serviceAccount: spark-instance-spark-service-account
  executor:
    instances: 10
    memory: "2g"
    labels:
      version: 3.0.0
      metrics-exposed: "true"
      app: sparky
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
    prometheus:
      jmxExporterJar: "/prometheus/jmx_prometheus_javaagent-0.11.0.jar"
      port: 8090
